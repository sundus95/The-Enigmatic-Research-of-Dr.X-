{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f712023d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " The Machine Readable file provides regression coefficients and intercepts for different components to calculate low, mid, and high material price estimates. The file provides a list of envelope and non-envelope components (e.g., Windows, Water Heaters) and any associated classes within those components.\n"
     ]
    }
   ],
   "source": [
    "#Abstractive Summarization with Hugging Face Transformers\n",
    "#Uses a pre-trained model like BART\n",
    "#Use this for natural language summaries (more human-like)\n",
    "\n",
    "import docx  # python-docx to handle DOCX files\n",
    "from transformers import pipeline\n",
    "\n",
    "# Function to extract text from DOCX\n",
    "def extract_docx_text(path):\n",
    "    doc = docx.Document(path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# Load the summarization model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Set your DOCX file path here\n",
    "docx_path = r\"C:\\Users\\salsubhi1\\PycharmProjects\\Enigmatic Research\\Dr.X Files\\Stats.docx\"  # Change this to your actual file path\n",
    "\n",
    "# Extract text from the DOCX file\n",
    "docx_text = extract_docx_text(docx_path)\n",
    "\n",
    "# Make sure the text isn't too short or too long for the model\n",
    "if len(docx_text) > 1024:\n",
    "    docx_text = docx_text[:1024]  # Limit to 1024 characters for demo (can be chunked for full doc)\n",
    "\n",
    "# Generate summary\n",
    "summary = summarizer(docx_text, max_length=150, min_length=50, do_sample=False)\n",
    "\n",
    "# Print the summary\n",
    "print(\"Summary:\\n\", summary[0]['summary_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57fa17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL SUMMARY:\n",
      "\n",
      "the machine readable file provides regression coefficients and intercepts for different components to calculate low, mid, and high material price estimates and labor multipliers/add-ons. the file provides a list of envelope and non-envelope components (e.g., Windows, Water Heaters) and any associated classes within those components (e.g., Low Emissivity, Electric Instantaneous) additional data The last section of the file contains additional data not directly within the calculation of each component and product class. these include the expected lifetime (in years) of the component, cost variation considerations, a list of data sources used in the analysis for each component. the labor cost is calculated by subtracting the material price from the installed cost. Example 2 Unfinished Attic Ceiling Batt Insulation (Retrofit Installation Adder) Example for calculating the low, mid, and high retail price along with the associated labor for replacing ceiling insulation in an unfinished attic with an R-Value of 15.\n"
     ]
    }
   ],
   "source": [
    "#T5 Model with Transformers\n",
    "#t5-small handles up to 512 tokens — not characters — so we split the text by word count.\n",
    "\n",
    "import docx  # python-docx to handle DOCX files\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# -------- Step 1: Extract text from DOCX --------\n",
    "def extract_docx_text(path):\n",
    "    doc = docx.Document(path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# -------- Step 2: Chunk text into 512-token pieces --------\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "# -------- Step 3: Load T5 model and tokenizer --------\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# -------- Step 4: Summarize each chunk --------\n",
    "def summarize_with_t5(text_chunks):\n",
    "    summaries = []\n",
    "    for chunk in text_chunks:\n",
    "        input_text = \"summarize: \" + chunk\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=150,\n",
    "            min_length=40,\n",
    "            length_penalty=2.0,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "# -------- Step 5: Put it all together --------\n",
    "docx_path = r\"C:\\Users\\salsubhi1\\PycharmProjects\\Enigmatic Research\\Dr.X Files\\Stats.docx\"  # Update this to your DOCX file\n",
    "full_text = extract_docx_text(docx_path)\n",
    "text_chunks = chunk_text(full_text)\n",
    "final_summary = summarize_with_t5(text_chunks)\n",
    "\n",
    "# -------- Print the result --------\n",
    "print(\"FINAL SUMMARY:\\n\")\n",
    "print(final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "542840e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRank Summary:\n",
      "\n",
      "Introduction The purpose of the Machine Readable file (“the file”) is to provide regression coefficients and intercepts for different components to calculate low, mid, and high (10th, 50th, and 90th percentile) material price estimates and labor multipliers/add-ons to estimate new construction and retrofit project costs.\n",
      "The second and third sections of the file (Retail Price Regression) show the “Coefficient-Low”, “Coefficient-Mid”, and “Coefficient-High” values that correspond to the low, mid, and high quantile regression coefficients that are used to multiply the chosen performance metric values.\n",
      "After getting the estimated material price from calculating the material price regression using the coefficients, intercepts, and chosen performance metric input values, the multiplier is used to calculate the total installed cost or cost per square foot.\n",
      "The numbers in red correspond to the different coefficients in the flat CSV file for the two performance metrics and the low, mid, and high regressions:\n",
      "The numbers in red correspond to the different coefficients in the Machine Readable CSV file for the performance metric and the low, mid, and high regressions:\n"
     ]
    }
   ],
   "source": [
    "#TextRank Summarization with Sumy\n",
    "import docx  # python-docx to handle DOCX files\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "# -------- Step 1: Extract text from DOCX --------\n",
    "def extract_docx_text(path):\n",
    "    doc = docx.Document(path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# -------- Step 2: Summarize the extracted text --------\n",
    "def summarize_docx_text(docx_path):\n",
    "    text = extract_docx_text(docx_path)\n",
    "    \n",
    "    # Use the TextRank Summarizer\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count=5)  # Number of sentences in the summary\n",
    "\n",
    "    # Output the summary\n",
    "    print(\"TextRank Summary:\\n\")\n",
    "    for sentence in summary:\n",
    "        print(sentence)\n",
    "\n",
    "# -------- Step 3: Run the summarization --------\n",
    "docx_path = r\"C:\\Users\\salsubhi1\\PycharmProjects\\Enigmatic Research\\Dr.X Files\\Stats.docx\"  # Update this path with your DOCX file\n",
    "summarize_docx_text(docx_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b24217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luhn Summary:\n",
      "\n",
      "Introduction The purpose of the Machine Readable file (“the file”) is to provide regression coefficients and intercepts for different components to calculate low, mid, and high (10th, 50th, and 90th percentile) material price estimates and labor multipliers/add-ons to estimate new construction and retrofit project costs.\n",
      "After getting the estimated material price from calculating the material price regression using the coefficients, intercepts, and chosen performance metric input values, the multiplier is used to calculate the total installed cost or cost per square foot.\n",
      "Prevailing local wages Drive time Access Presence/condition/type of existing insulation Existing construction and materials Moisture issues present Condition of existing flue Need for condensate line/drain Need to bring in combustion air Condition of existing electrical system Presence of hazardous materials Nature/size of leaks Extent of preparation Each regression was given a confidence rating in the categories of sample size (SS), median  (R2), and source diversity, to qualify how robust the data and corresponding regressions are.\n",
      "Price Calculation Example Example 1: Air Source Heat Pump (Retrofit Installation Multiplier) Example for calculating the low, mid, and high retail price along with the associated labor for replacing an air source heat pump that does not require a new circuit or panel upgrade.\n",
      "Example 2 Unfinished Attic Ceiling Batt Insulation (Retrofit Installation Adder) Example for calculating the low, mid, and high retail price along with the associated labor for replacing (retrofitting) ceiling insulation in an unfinished attic with an R-Value of 15, using fiberglass batt insulation.\n"
     ]
    }
   ],
   "source": [
    "#Luhn Summarizer (Sumy)\n",
    "import docx  # python-docx to handle DOCX files\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "\n",
    "# -------- Step 1: Extract text from DOCX --------\n",
    "def extract_docx_text(path):\n",
    "    doc = docx.Document(path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# -------- Step 2: Summarize the extracted text --------\n",
    "def summarize_docx_text(docx_path):\n",
    "    text = extract_docx_text(docx_path)\n",
    "    \n",
    "    # Use the Luhn Summarizer\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LuhnSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count=5)  # Number of sentences in the summary\n",
    "\n",
    "    # Output the summary\n",
    "    print(\"Luhn Summary:\\n\")\n",
    "    for sentence in summary:\n",
    "        print(sentence)\n",
    "\n",
    "# -------- Step 3: Run the summarization --------\n",
    "docx_path = r\"C:\\Users\\salsubhi1\\PycharmProjects\\Enigmatic Research\\Dr.X Files\\Stats.docx\"  # Update this path with your DOCX file\n",
    "summarize_docx_text(docx_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e926c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy Custom Extractive Summary:\n",
      "\n",
      "Prevailing local wages\n",
      "Drive time\n",
      "Access\n",
      "Presence/condition/type of existing insulation\n",
      "Existing construction and materials\n",
      "Moisture issues present\n",
      "Condition of existing flue\n",
      "Need for condensate line/drain\n",
      "Need to bring in combustion air\n",
      "Condition of existing electrical system\n",
      "Presence of hazardous materials\n",
      "Nature/size of leaks\n",
      "Extent of preparation\n",
      "Each regression was given a confidence rating in the categories of sample size (SS), median  (R2), and source diversity, to qualify how robust the data and corresponding regressions are.\n",
      "The numbers in red correspond to the different coefficients in the flat CSV file for the two performance metrics and the low, mid, and high regressions:\n",
      "\n",
      "Where A is the capacity in tons, B is the efficiency in SEER1, and C is the intercept value (constant).\n",
      "Introduction\n",
      "The purpose of the Machine Readable file (“the file”) is to provide regression coefficients and intercepts for different components to calculate low, mid, and high (10th, 50th, and 90th percentile) material price estimates and labor multipliers/add-ons to estimate new construction and retrofit project costs.\n",
      "The second and third sections of the file (Retail Price Regression) show the “Coefficient-Low”, “Coefficient-Mid”, and “Coefficient-High” values that correspond to the low, mid, and high quantile regression coefficients that are used to multiply the chosen performance metric values.\n",
      "These include the expected lifetime (in years) of the component, cost variation considerations, a list of data sources used in the analysis for each component (using a numbering format), and a qualitative confidence rating of the data.\n"
     ]
    }
   ],
   "source": [
    "#SpaCy + Sentence Scoring (Custom Extractive)\n",
    "import docx  # python-docx to handle DOCX files\n",
    "import spacy\n",
    "\n",
    "# -------- Step 1: Extract text from DOCX --------\n",
    "def extract_docx_text(path):\n",
    "    doc = docx.Document(path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "# -------- Step 2: Summarize the extracted text using SpaCy --------\n",
    "def summarize_docx_text(docx_path):\n",
    "    # Load the SpaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Extract text from the DOCX file\n",
    "    text = extract_docx_text(docx_path)\n",
    "    \n",
    "    # Process the text with SpaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Dictionary to store sentence scores\n",
    "    sentence_scores = {}\n",
    "\n",
    "    # Simple heuristic: score = named entities + noun chunks\n",
    "    for sent in doc.sents:\n",
    "        score = len(list(sent.ents)) + len(list(sent.noun_chunks))\n",
    "        sentence_scores[sent] = score\n",
    "\n",
    "    # Get top 5 sentences based on the highest score\n",
    "    top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:5]\n",
    "\n",
    "    # Output the top sentences\n",
    "    print(\"SpaCy Custom Extractive Summary:\\n\")\n",
    "    for sentence in top_sentences:\n",
    "        print(sentence.text.strip())\n",
    "\n",
    "# -------- Step 3: Run the summarization --------\n",
    "docx_path = r\"C:\\Users\\salsubhi1\\PycharmProjects\\Enigmatic Research\\Dr.X Files\\Stats.docx\"  # Update with your DOCX file path\n",
    "summarize_docx_text(docx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693c2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620255f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
